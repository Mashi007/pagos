# üîç AN√ÅLISIS: PROBLEMA DE RENDIMIENTO EN DASHBOARD

**Fecha:** 2025-11-06  
**Problema:** Tiempos de respuesta muy altos (1-5+ segundos) en endpoints del dashboard

---

## üìä S√çNTOMAS OBSERVADOS

### Logs de Producci√≥n (2025-11-06T02:00:11Z)

**M√∫ltiples peticiones simult√°neas del dashboard:**
- `/api/v1/dashboard/kpis-principales` ‚Üí 822ms
- `/api/v1/dashboard/opciones-filtros` ‚Üí 346ms
- `/api/v1/dashboard/prestamos-por-concesionario` ‚Üí 759ms
- `/api/v1/dashboard/financiamiento-tendencia-mensual` ‚Üí 1225ms
- `/api/v1/dashboard/pagos-conciliados` ‚Üí 1445ms
- `/api/v1/dashboard/evolucion-morosidad` ‚Üí 1668ms
- `/api/v1/dashboard/prestamos-por-modelo` ‚Üí 1950ms
- `/api/v1/dashboard/cobranzas-semanales` ‚Üí 1822ms
- `/api/v1/dashboard/admin` ‚Üí 2948ms
- `/api/v1/dashboard/financiamiento-por-rangos` ‚Üí 2941ms
- `/api/v1/dashboard/cobranzas-mensuales` ‚Üí 3214ms
- `/api/v1/dashboard/resumen-financiamiento-pagado` ‚Üí 3146ms
- `/api/v1/dashboard/morosidad-por-analista` ‚Üí 3225ms
- `/api/v1/dashboard/composicion-morosidad` ‚Üí 3590ms
- `/api/v1/dashboard/evolucion-pagos` ‚Üí 4598ms
- `/api/v1/dashboard/financiamiento-por-rangos` ‚Üí 5097ms
- `/api/v1/dashboard/admin` ‚Üí 5161ms

**Total: 17 endpoints ejecut√°ndose simult√°neamente en ~2 segundos**

---

## üî¥ PROBLEMA PRINCIPAL

### 1. **Cache No Funcional en Producci√≥n**

**Causa Ra√≠z:**
- El sistema usa `MemoryCache` (cache en memoria) porque Redis no est√° disponible
- **MemoryCache NO funciona con m√∫ltiples workers** en producci√≥n:
  - Cada worker de Gunicorn tiene su propia memoria
  - No hay sincronizaci√≥n entre workers
  - El cache se duplica innecesariamente
  - Cada worker calcula los mismos datos independientemente

**Evidencia:**
```python
# backend/app/core/cache.py
cache_backend: CacheBackend = MemoryCache()  # Fallback cuando Redis falla
```

**Logs esperados (si Redis funcionara):**
```
‚úÖ Redis cache inicializado correctamente
```

**Logs actuales (producci√≥n):**
```
‚ö†Ô∏è Usando MemoryCache - NO recomendado para producci√≥n con m√∫ltiples workers
```

### 2. **M√∫ltiples Peticiones Simult√°neas**

**Problema:**
- El frontend carga el dashboard y hace **17 peticiones API simult√°neas**
- Todas son cache MISS (primera carga o cache expirado)
- Todas ejecutan queries complejas a la BD al mismo tiempo
- Esto satura la base de datos

**Impacto:**
- 17 queries complejas ejecut√°ndose simult√°neamente
- Sin cache compartido, cada worker calcula todo desde cero
- Tiempo de respuesta total: **5+ segundos**

### 3. **Redis No Instalado en Producci√≥n**

**Evidencia:**
```python
# backend/requirements/prod.txt
# redis==5.0.1  # COMENTADO - No instalado
```

**Consecuencia:**
- El sistema siempre usa MemoryCache como fallback
- Cache no funciona entre workers
- Cada petici√≥n calcula todo desde cero

---

## ‚úÖ SOLUCIONES PROPUESTAS

### **SOLUCI√ìN 1: Configurar Redis (CR√çTICO - PRIORIDAD ALTA)**

**Impacto:** 80-95% reducci√≥n de tiempo de respuesta  
**Complejidad:** Baja  
**Tiempo:** 30 minutos

#### Pasos:

1. **Instalar Redis en Render.com:**
   - Crear servicio Redis en Render
   - Obtener URL de conexi√≥n

2. **Actualizar requirements/prod.txt:**
   ```python
   # Descomentar Redis
   redis==5.0.1
   ```

3. **Configurar variables de entorno en Render:**
   ```bash
   REDIS_URL=redis://default:password@redis-host:6379
   # O componentes individuales:
   REDIS_HOST=your-redis-service.onrender.com
   REDIS_PORT=6379
   REDIS_PASSWORD=your-password
   REDIS_DB=0
   REDIS_SOCKET_TIMEOUT=5
   ```

4. **Verificar logs despu√©s del deploy:**
   ```
   ‚úÖ Redis cache inicializado correctamente
   üîó Conectando a Redis usando REDIS_URL: ...
   ```

**Resultado Esperado:**
- Primera carga: 5 segundos (calcula y cachea)
- Cargas siguientes (5 min): <100ms (cache HIT)
- **Mejora: 95% menos tiempo de respuesta**

---

### **SOLUCI√ìN 2: Optimizar Carga del Frontend (MEDIO PLAZO)**

**Impacto:** 50-70% reducci√≥n de carga inicial  
**Complejidad:** Media  
**Tiempo:** 2-3 horas

#### Opci√≥n A: Carga Paralela con L√≠mite

Limitar peticiones simult√°neas a 3-5 en lugar de 17:

```typescript
// frontend/src/components/Dashboard.tsx
// Cargar en batches de 5 peticiones
const loadDashboardData = async () => {
  const batch1 = await Promise.all([
    fetchKpis(),
    fetchOpcionesFiltros(),
    fetchPrestamosConcesionario(),
    fetchFinanciamientoTendencia(),
    fetchPagosConciliados(),
  ]);
  
  // Esperar 200ms antes del siguiente batch
  await new Promise(resolve => setTimeout(resolve, 200));
  
  const batch2 = await Promise.all([
    fetchEvolucionMorosidad(),
    // ... m√°s peticiones
  ]);
  
  // ... m√°s batches
};
```

#### Opci√≥n B: Carga Secuencial con Prioridad

Cargar primero los KPIs m√°s importantes, luego el resto:

```typescript
// 1. Cargar KPIs principales primero (cr√≠ticos)
const kpis = await fetchKpis();

// 2. Cargar opciones de filtros (r√°pido)
const filtros = await fetchOpcionesFiltros();

// 3. Cargar gr√°ficos en paralelo (menos cr√≠ticos)
const [graficos1, graficos2, graficos3] = await Promise.all([
  fetchEvolucionMorosidad(),
  fetchEvolucionPagos(),
  fetchPrestamosModelo(),
]);
```

**Resultado Esperado:**
- KPIs principales visibles en <1 segundo
- Resto del dashboard carga progresivamente
- Menos saturaci√≥n de BD

---

### **SOLUCI√ìN 3: Aumentar TTL del Cache (R√ÅPIDO)**

**Impacto:** 30-50% reducci√≥n de regeneraci√≥n de cache  
**Complejidad:** Muy Baja  
**Tiempo:** 5 minutos

**Cambio:**
```python
# backend/app/api/v1/endpoints/dashboard.py

# ANTES: 5 minutos (300 segundos)
@cache_result(ttl=300, key_prefix="dashboard")

# DESPU√âS: 10 minutos (600 segundos) para datos menos cr√≠ticos
@cache_result(ttl=600, key_prefix="dashboard")

# Para opciones-filtros (cambian muy poco): 30 minutos
@cache_result(ttl=1800, key_prefix="dashboard")
```

**Endpoints a ajustar:**
- `opciones-filtros`: 1800s (30 min) - Cambian muy poco
- `kpis-principales`: 600s (10 min) - Balance entre frescura y rendimiento
- `evolucion-morosidad`: 600s (10 min) - Datos hist√≥ricos
- `prestamos-por-concesionario`: 600s (10 min)

**Resultado Esperado:**
- Cache dura m√°s tiempo
- Menos regeneraciones innecesarias
- Menos carga en BD

---

### **SOLUCI√ìN 4: Query Batching (LARGO PLAZO)**

**Impacto:** 60-80% reducci√≥n de queries  
**Complejidad:** Alta  
**Tiempo:** 8-12 horas

**Idea:**
Crear un endpoint √∫nico que devuelva todos los datos del dashboard en una sola petici√≥n:

```python
@router.get("/dashboard/completo")
@cache_result(ttl=300, key_prefix="dashboard")
def obtener_dashboard_completo(
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user),
):
    """
    Devuelve todos los datos del dashboard en una sola petici√≥n
    Optimizado para reducir queries y aprovechar cache
    """
    # Ejecutar queries en paralelo dentro del servidor
    # Retornar todo en un solo JSON
    return {
        "kpis": calcular_kpis(),
        "filtros": obtener_opciones_filtros(),
        "prestamos_concesionario": calcular_prestamos_concesionario(),
        # ... resto de datos
    }
```

**Resultado Esperado:**
- 1 petici√≥n en lugar de 17
- Cache compartido entre todos los datos
- Menos overhead de red

---

## üìã PLAN DE ACCI√ìN RECOMENDADO

### **FASE 1: INMEDIATO (Hoy)**
1. ‚úÖ **Configurar Redis en Render** (30 min)
   - Crear servicio Redis
   - Configurar variables de entorno
   - Descomentar Redis en requirements/prod.txt
   - Deploy y verificar logs

2. ‚úÖ **Aumentar TTL del cache** (5 min)
   - Ajustar TTLs seg√∫n criticidad
   - Deploy

**Resultado Esperado:** 80-95% mejora en tiempo de respuesta

---

### **FASE 2: CORTO PLAZO (Esta Semana)**
3. ‚úÖ **Optimizar carga del frontend** (2-3 horas)
   - Implementar carga por batches
   - Priorizar KPIs cr√≠ticos
   - Deploy frontend

**Resultado Esperado:** 50-70% reducci√≥n de carga inicial

---

### **FASE 3: MEDIO PLAZO (Pr√≥ximas 2 Semanas)**
4. ‚è≥ **Query Batching** (8-12 horas)
   - Crear endpoint consolidado
   - Optimizar queries
   - Actualizar frontend

**Resultado Esperado:** 60-80% reducci√≥n de queries totales

---

## üìä M√âTRICAS DE √âXITO

### Antes (Estado Actual):
- Tiempo de carga inicial: **5+ segundos**
- Cache hits: **0%** (MemoryCache no funciona)
- Peticiones simult√°neas: **17**
- Queries a BD por carga: **17+ queries complejas**

### Despu√©s (Fase 1 - Redis):
- Tiempo de carga inicial: **5 segundos** (primera vez)
- Tiempo de carga siguiente: **<500ms** (cache HIT)
- Cache hits: **>90%** despu√©s de primera carga
- Peticiones simult√°neas: **17** (igual, pero con cache)
- Queries a BD por carga: **17** (solo primera vez, luego 0)

### Despu√©s (Fase 2 - Optimizaci√≥n Frontend):
- Tiempo de carga inicial: **2-3 segundos** (carga progresiva)
- KPIs visibles en: **<1 segundo**
- Peticiones simult√°neas: **3-5** (batches)
- Queries a BD por carga: **3-5** (solo primera vez)

### Despu√©s (Fase 3 - Query Batching):
- Tiempo de carga inicial: **2 segundos** (una sola petici√≥n)
- Cache hits: **>95%**
- Peticiones simult√°neas: **1**
- Queries a BD por carga: **1 batch optimizado**

---

## üö® PRIORIDADES

### **CR√çTICO (Hacer Hoy):**
1. ‚úÖ Configurar Redis en Render
2. ‚úÖ Aumentar TTL del cache

### **IMPORTANTE (Esta Semana):**
3. ‚úÖ Optimizar carga del frontend

### **MEJORA (Pr√≥ximas 2 Semanas):**
4. ‚è≥ Query Batching

---

## üìù NOTAS ADICIONALES

### ¬øPor qu√© MemoryCache no funciona en producci√≥n?

**Gunicorn con m√∫ltiples workers:**
```
Worker 1: MemoryCache (vac√≠o)
Worker 2: MemoryCache (vac√≠o)
Worker 3: MemoryCache (vac√≠o)
```

**Flujo sin Redis:**
1. Usuario hace petici√≥n ‚Üí Worker 1
2. Worker 1: Cache MISS ‚Üí Calcula ‚Üí Guarda en su MemoryCache
3. Usuario hace petici√≥n ‚Üí Worker 2 (diferente worker)
4. Worker 2: Cache MISS ‚Üí Calcula ‚Üí Guarda en su MemoryCache
5. **Resultado:** Cada worker calcula todo independientemente

**Flujo con Redis:**
1. Usuario hace petici√≥n ‚Üí Worker 1
2. Worker 1: Cache MISS ‚Üí Calcula ‚Üí Guarda en Redis
3. Usuario hace petici√≥n ‚Üí Worker 2
4. Worker 2: Cache HIT ‚Üí Obtiene de Redis ‚Üí Respuesta instant√°nea
5. **Resultado:** Todos los workers comparten el mismo cache

---

## ‚úÖ CONCLUSI√ìN

El problema principal es que **Redis no est√° configurado en producci√≥n**, lo que causa que el cache no funcione entre m√∫ltiples workers. Esto resulta en:

- **5+ segundos** de tiempo de respuesta
- **17 queries simult√°neas** a la BD
- **0% de cache hits** efectivos
- **Saturaci√≥n de la base de datos**

**La soluci√≥n inmediata es configurar Redis**, lo que deber√≠a reducir el tiempo de respuesta en **80-95%** para cargas subsecuentes.

---

**Generado:** 2025-11-06  
**Estado:** An√°lisis Completo - Listo para Implementaci√≥n

