{"dependencies": [{"name": "aiohappyeyeballs", "version": "2.6.1", "vulns": []}, {"name": "aiohttp", "version": "3.13.1", "vulns": [{"id": "CVE-2025-69223", "fix_versions": ["3.13.3"], "aliases": ["GHSA-6mq8-rvhq-8wgg"], "description": "### Summary A zip bomb can be used to execute a DoS against the aiohttp server.  ### Impact An attacker may be able to send a compressed request that when decompressed by aiohttp could exhaust the host's memory.  ------  Patch: https://github.com/aio-libs/aiohttp/commit/2b920c39002cee0ec5b402581779bbaaf7c9138a"}, {"id": "CVE-2025-69224", "fix_versions": ["3.13.3"], "aliases": ["GHSA-69f9-5gxw-wvc2"], "description": "### Summary The Python HTTP parser may allow a request smuggling attack with the presence of non-ASCII characters.  ### Impact If a pure Python version of aiohttp is installed (i.e. without the usual C extensions) or AIOHTTP_NO_EXTENSIONS is enabled, then an attacker may be able to execute a request smuggling attack to bypass certain firewalls or proxy protections.  ------  Patch: https://github.com/aio-libs/aiohttp/commit/32677f2adfd907420c078dda6b79225c6f4ebce0"}, {"id": "CVE-2025-69228", "fix_versions": ["3.13.3"], "aliases": ["GHSA-6jhg-hg63-jvvf"], "description": "### Summary A request can be crafted in such a way that an aiohttp server's memory fills up uncontrollably during processing.  ### Impact If an application includes a handler that uses the `Request.post()` method, an attacker may be able to freeze the server by exhausting the memory.  -----  Patch: https://github.com/aio-libs/aiohttp/commit/b7dbd35375aedbcd712cbae8ad513d56d11cce60"}, {"id": "CVE-2025-69229", "fix_versions": ["3.13.3"], "aliases": ["GHSA-g84x-mcqj-x9qq"], "description": "### Summary  Handling of chunked messages can result in excessive blocking CPU usage when receiving a large number of chunks.  ### Impact  If an application makes use of the `request.read()` method in an endpoint, it may be possible for an attacker to cause the server to spend a moderate amount of blocking CPU time (e.g. 1 second) while processing the request. This could potentially lead to DoS as the server would be unable to handle other requests during that time.  -----  Patch: https://github.com/aio-libs/aiohttp/commit/dc3170b56904bdf814228fae70a5501a42a6c712 Patch: https://github.com/aio-libs/aiohttp/commit/4ed97a4e46eaf61bd0f05063245f613469700229"}, {"id": "CVE-2025-69230", "fix_versions": ["3.13.3"], "aliases": ["GHSA-fh55-r93g-j68g"], "description": "### Summary Reading multiple invalid cookies can lead to a logging storm.  ### Impact If the ``cookies`` attribute is accessed in an application, then an attacker may be able to trigger a storm of warning-level logs using a specially crafted Cookie header.  ----  Patch: https://github.com/aio-libs/aiohttp/commit/64629a0834f94e46d9881f4e99c41a137e1f3326"}, {"id": "CVE-2025-69226", "fix_versions": ["3.13.3"], "aliases": ["GHSA-54jq-c3m8-4m76"], "description": "### Summary Path normalization for static files prevents path traversal, but opens up the ability for an attacker to ascertain the existence of absolute path components.  ### Impact If an application uses `web.static()` (not recommended for production deployments), it may be possible for an attacker to ascertain the existence of path components.  ------  Patch: https://github.com/aio-libs/aiohttp/commit/f2a86fd5ac0383000d1715afddfa704413f0711e"}, {"id": "CVE-2025-69227", "fix_versions": ["3.13.3"], "aliases": ["GHSA-jj3x-wxrx-4x23"], "description": "### Summary When assert statements are bypassed, an infinite loop can occur, resulting in a DoS attack when processing a POST body.  ### Impact If optimisations are enabled (`-O` or `PYTHONOPTIMIZE=1`), and the application includes a handler that uses the `Request.post()` method, then an attacker may be able to execute a DoS attack with a specially crafted message.  ------  Patch: https://github.com/aio-libs/aiohttp/commit/bc1319ec3cbff9438a758951a30907b072561259"}, {"id": "CVE-2025-69225", "fix_versions": ["3.13.3"], "aliases": ["GHSA-mqqc-3gqh-h2x8"], "description": "### Summary  The parser allows non-ASCII decimals to be present in the Range header.  ### Impact  There is no known impact, but there is the possibility that there's a method to exploit a request smuggling vulnerability.  ----  Patch: https://github.com/aio-libs/aiohttp/commit/c7b7a044f88c71cefda95ec75cdcfaa4792b3b96"}]}, {"name": "aiosignal", "version": "1.4.0", "vulns": []}, {"name": "aiosmtplib", "version": "5.0.0", "vulns": []}, {"name": "alembic", "version": "1.17.0", "vulns": []}, {"name": "annotated-doc", "version": "0.0.2", "vulns": []}, {"name": "annotated-types", "version": "0.7.0", "vulns": []}, {"name": "anyio", "version": "4.9.0", "vulns": []}, {"name": "attrs", "version": "25.4.0", "vulns": []}, {"name": "autoflake", "version": "2.3.1", "vulns": []}, {"name": "autopep8", "version": "2.3.2", "vulns": []}, {"name": "bcrypt", "version": "5.0.0", "vulns": []}, {"name": "beautifulsoup4", "version": "4.13.4", "vulns": []}, {"name": "black", "version": "25.9.0", "vulns": []}, {"name": "boolean-py", "version": "5.0", "vulns": []}, {"name": "cachecontrol", "version": "0.14.4", "vulns": []}, {"name": "certifi", "version": "2025.4.26", "vulns": []}, {"name": "cffi", "version": "2.0.0", "vulns": []}, {"name": "charset-normalizer", "version": "3.4.2", "vulns": []}, {"name": "click", "version": "8.2.1", "vulns": []}, {"name": "colorama", "version": "0.4.6", "vulns": []}, {"name": "cryptography", "version": "46.0.3", "vulns": []}, {"name": "cyclonedx-python-lib", "version": "11.6.0", "vulns": []}, {"name": "defusedxml", "version": "0.7.1", "vulns": []}, {"name": "deprecated", "version": "1.3.1", "vulns": []}, {"name": "dnspython", "version": "2.8.0", "vulns": []}, {"name": "ecdsa", "version": "0.19.1", "vulns": [{"id": "CVE-2024-23342", "fix_versions": [], "aliases": ["GHSA-wj6h-64fc-37mp"], "description": "python-ecdsa has been found to be subject to a Minerva timing attack on the P-256 curve. Using the `ecdsa.SigningKey.sign_digest()` API function and timing signatures an attacker can leak the internal nonce which may allow for private key discovery. Both ECDSA signatures, key generation, and ECDH operations are affected. ECDSA signature verification is unaffected. The python-ecdsa project considers side channel attacks out of scope for the project and there is no planned fix."}]}, {"name": "email-validator", "version": "2.3.0", "vulns": []}, {"name": "et-xmlfile", "version": "2.0.0", "vulns": []}, {"name": "fastapi", "version": "0.120.0", "vulns": []}, {"name": "filelock", "version": "3.20.3", "vulns": []}, {"name": "flake8", "version": "7.3.0", "vulns": []}, {"name": "frozenlist", "version": "1.8.0", "vulns": []}, {"name": "git-filter-repo", "version": "2.47.0", "vulns": []}, {"name": "greenlet", "version": "3.2.4", "vulns": []}, {"name": "h11", "version": "0.16.0", "vulns": []}, {"name": "httpcore", "version": "1.0.9", "vulns": []}, {"name": "httpx", "version": "0.28.1", "vulns": []}, {"name": "httpx-sse", "version": "0.4.0", "vulns": []}, {"name": "idna", "version": "3.10", "vulns": []}, {"name": "isort", "version": "7.0.0", "vulns": []}, {"name": "jinja2", "version": "3.1.6", "vulns": []}, {"name": "joblib", "version": "1.5.2", "vulns": []}, {"name": "license-expression", "version": "30.4.4", "vulns": []}, {"name": "limits", "version": "5.6.0", "vulns": []}, {"name": "mako", "version": "1.3.10", "vulns": []}, {"name": "markdown-it-py", "version": "4.0.0", "vulns": []}, {"name": "markupsafe", "version": "3.0.3", "vulns": []}, {"name": "mccabe", "version": "0.7.0", "vulns": []}, {"name": "mcp", "version": "1.9.4", "vulns": [{"id": "CVE-2025-53365", "fix_versions": ["1.10.0"], "aliases": ["GHSA-j975-95f5-7wqh"], "description": "If a client deliberately triggers an exception after establishing a streamable HTTP session, this can lead to an uncaught ClosedResourceError on the server side, causing the server to crash and requiring a restart to restore service. Impact may vary depending on the deployment conditions, and presence of infrastructure-level resilience measures.  Thank you to Rich Harang for reporting this issue."}, {"id": "CVE-2025-66416", "fix_versions": ["1.23.0"], "aliases": ["GHSA-9h52-p55h-vw2f"], "description": "### Description  The Model Context Protocol (MCP) Python SDK does not enable DNS rebinding protection by default for HTTP-based servers. When an HTTP-based MCP server is run on localhost without authentication using `FastMCP` with streamable HTTP or SSE transport, and has not configured `TransportSecuritySettings`, a malicious website could exploit DNS rebinding to bypass same-origin policy restrictions and send requests to the local MCP server. This could allow an attacker to invoke tools or access resources exposed by the MCP server on behalf of the user in those limited circumstances.  Note that running HTTP-based MCP servers locally without authentication is not recommended per MCP security best practices. This issue does not affect servers using stdio transport.  Servers created via `FastMCP()` now have DNS rebinding protection enabled by default when the `host` parameter is `127.0.0.1` or `localhost`. Users are advised to update to version `1.23.0` to receive this automatic protection. Users with custom low-level server configurations using `StreamableHTTPSessionManager` or `SseServerTransport` directly should explicitly configure `TransportSecuritySettings` when running an unauthenticated server on localhost."}]}, {"name": "mdurl", "version": "0.1.2", "vulns": []}, {"name": "msgpack", "version": "1.1.2", "vulns": []}, {"name": "multidict", "version": "6.7.0", "vulns": []}, {"name": "mypy", "version": "1.18.2", "vulns": []}, {"name": "mypy-extensions", "version": "1.1.0", "vulns": []}, {"name": "numpy", "version": "2.2.6", "vulns": []}, {"name": "openpyxl", "version": "3.1.5", "vulns": []}, {"name": "packageurl-python", "version": "0.17.6", "vulns": []}, {"name": "packaging", "version": "25.0", "vulns": []}, {"name": "pandas", "version": "2.2.3", "vulns": []}, {"name": "passlib", "version": "1.7.4", "vulns": []}, {"name": "pathspec", "version": "0.12.1", "vulns": []}, {"name": "pillow", "version": "12.0.0", "vulns": []}, {"name": "pip", "version": "25.1.1", "vulns": [{"id": "CVE-2025-8869", "fix_versions": ["25.3"], "aliases": ["GHSA-4xh5-x5gv-qwph"], "description": "When extracting a tar archive pip may not check symbolic links point into the extraction directory if the tarfile module doesn't implement PEP 706. Note that upgrading pip to a \"fixed\" version for this vulnerability doesn't fix all known vulnerabilities that are remediated by using a Python version that implements PEP 706. Note that this is a vulnerability in pip's fallback implementation of tar extraction for Python versions that don't implement PEP 706 and therefore are not secure to all vulnerabilities in the Python 'tarfile' module. If you're using a Python version that implements PEP 706 then pip doesn't use the \"vulnerable\" fallback code. Mitigations include upgrading to a version of pip that includes the fix, upgrading to a Python version that implements PEP 706 (Python >=3.9.17, >=3.10.12, >=3.11.4, or >=3.12), applying the linked patch, or inspecting source distributions (sdists) before installation as is already a best-practice."}]}, {"name": "pip-api", "version": "0.0.34", "vulns": []}, {"name": "pip-audit", "version": "2.10.0", "vulns": []}, {"name": "pip-requirements-parser", "version": "32.0.1", "vulns": []}, {"name": "platformdirs", "version": "4.5.0", "vulns": []}, {"name": "playwright", "version": "1.55.0", "vulns": []}, {"name": "propcache", "version": "0.4.1", "vulns": []}, {"name": "psutil", "version": "6.1.0", "vulns": []}, {"name": "psycopg2-binary", "version": "2.9.11", "vulns": []}, {"name": "py-serializable", "version": "2.1.0", "vulns": []}, {"name": "pyasn1", "version": "0.6.1", "vulns": []}, {"name": "pycodestyle", "version": "2.14.0", "vulns": []}, {"name": "pycparser", "version": "2.23", "vulns": []}, {"name": "pydantic", "version": "2.11.7", "vulns": []}, {"name": "pydantic-core", "version": "2.33.2", "vulns": []}, {"name": "pydantic-settings", "version": "2.9.1", "vulns": []}, {"name": "pyee", "version": "13.0.0", "vulns": []}, {"name": "pyflakes", "version": "3.4.0", "vulns": []}, {"name": "pygments", "version": "2.19.2", "vulns": []}, {"name": "pyjwt", "version": "2.10.1", "vulns": []}, {"name": "pyparsing", "version": "3.3.1", "vulns": []}, {"name": "python-dateutil", "version": "2.8.2", "vulns": []}, {"name": "python-dotenv", "version": "1.1.0", "vulns": []}, {"name": "python-jose", "version": "3.5.0", "vulns": []}, {"name": "python-multipart", "version": "0.0.20", "vulns": []}, {"name": "pytokens", "version": "0.2.0", "vulns": []}, {"name": "pytz", "version": "2024.1", "vulns": []}, {"name": "reportlab", "version": "4.4.4", "vulns": []}, {"name": "requests", "version": "2.32.5", "vulns": []}, {"name": "rich", "version": "14.2.0", "vulns": []}, {"name": "rsa", "version": "4.9.1", "vulns": []}, {"name": "scikit-learn", "version": "1.6.1", "vulns": []}, {"name": "scipy", "version": "1.16.3", "vulns": []}, {"name": "six", "version": "1.17.0", "vulns": []}, {"name": "slowapi", "version": "0.1.9", "vulns": []}, {"name": "sniffio", "version": "1.3.1", "vulns": []}, {"name": "sortedcontainers", "version": "2.4.0", "vulns": []}, {"name": "soupsieve", "version": "2.7", "vulns": []}, {"name": "sqlalchemy", "version": "2.0.44", "vulns": []}, {"name": "sse-starlette", "version": "2.3.6", "vulns": []}, {"name": "starlette", "version": "0.47.1", "vulns": [{"id": "CVE-2025-54121", "fix_versions": ["0.47.2"], "aliases": ["GHSA-2c2j-9gv5-cj73"], "description": "### Summary When parsing a multi-part form with large files (greater than the [default max spool size](https://github.com/encode/starlette/blob/fa5355442753f794965ae1af0f87f9fec1b9a3de/starlette/formparsers.py#L126)) `starlette` will block the main thread to roll the file over to disk. This blocks the event thread which means we can't accept new connections.  ### Details Please see this discussion for details: https://github.com/encode/starlette/discussions/2927#discussioncomment-13721403. In summary the following UploadFile code (copied from [here](https://github.com/encode/starlette/blob/fa5355442753f794965ae1af0f87f9fec1b9a3de/starlette/datastructures.py#L436C5-L447C14)) has a minor bug. Instead of just checking for `self._in_memory` we should also check if the additional bytes will cause a rollover.  ```python      @property     def _in_memory(self) -> bool:         # check for SpooledTemporaryFile._rolled         rolled_to_disk = getattr(self.file, \"_rolled\", True)         return not rolled_to_disk      async def write(self, data: bytes) -> None:         if self.size is not None:             self.size += len(data)          if self._in_memory:             self.file.write(data)         else:             await run_in_threadpool(self.file.write, data) ```  I have already created a PR which fixes the problem: https://github.com/encode/starlette/pull/2962   ### PoC See the discussion [here](https://github.com/encode/starlette/discussions/2927#discussioncomment-13721403) for steps on how to reproduce.  ### Impact To be honest, very low and not many users will be impacted. Parsing large forms is already CPU intensive so the additional IO block doesn't slow down `starlette` that much on systems with modern HDDs/SSDs. If someone is running on tape they might see a greater impact."}, {"id": "CVE-2025-62727", "fix_versions": ["0.49.1"], "aliases": ["GHSA-7f5h-v6xp-fcq8"], "description": "### Summary An unauthenticated attacker can send a crafted HTTP Range header that triggers quadratic-time processing in Starlette's `FileResponse` Range parsing/merging logic. This enables CPU exhaustion per request, causing denial\u2011of\u2011service for endpoints serving files (e.g., `StaticFiles` or any use of `FileResponse`).  ### Details Starlette parses multi-range requests in ``FileResponse._parse_range_header()``, then merges ranges using an O(n^2) algorithm.  ```python # starlette/responses.py _RANGE_PATTERN = re.compile(r\"(\\d*)-(\\d*)\") # vulnerable to O(n^2) complexity ReDoS  class FileResponse(Response):     @staticmethod     def _parse_range_header(http_range: str, file_size: int) -> list[tuple[int, int]]:         ranges: list[tuple[int, int]] = []         try:             units, range_ = http_range.split(\"=\", 1)         except ValueError:             raise MalformedRangeHeader()          # [...]          ranges = [             (                 int(_[0]) if _[0] else file_size - int(_[1]),                 int(_[1]) + 1 if _[0] and _[1] and int(_[1]) < file_size else file_size,             )             for _ in _RANGE_PATTERN.findall(range_) # vulnerable             if _ != (\"\", \"\")         ]  ```  The parsing loop of ``FileResponse._parse_range_header()`` uses the regular expression which vulnerable to denial of service for its O(n^2) complexity. A crafted `Range` header can maximize its complexity.  The merge loop processes each input range by scanning the entire result list, yielding quadratic behavior with many disjoint ranges. A crafted Range header with many small, non-overlapping ranges (or specially shaped numeric substrings) maximizes comparisons.    This affects any Starlette application that uses:    - ``starlette.staticfiles.StaticFiles`` (internally returns `FileResponse`) \u2014 `starlette/staticfiles.py:178`   - Direct ``starlette.responses.FileResponse`` responses  ### PoC ```python #!/usr/bin/env python3  import sys import time  try:     import starlette     from starlette.responses import FileResponse except Exception as e:     print(f\"[ERROR] Failed to import starlette: {e}\")     sys.exit(1)   def build_payload(length: int) -> str:     \"\"\"Build the Range header value body: '0' * num_zeros + '0-'\"\"\"     return (\"0\" * length) + \"a-\"   def test(header: str, file_size: int) -> float:     start = time.perf_counter()     try:         FileResponse._parse_range_header(header, file_size)     except Exception:         pass     end = time.perf_counter()     elapsed = end - start     return elapsed   def run_once(num_zeros: int) -> None:     range_body = build_payload(num_zeros)     header = \"bytes=\" + range_body     # Use a sufficiently large file_size so upper bounds default to file size     file_size = max(len(range_body) + 10, 1_000_000)          print(f\"[DEBUG] range_body length: {len(range_body)} bytes\")     elapsed_time = test(header, file_size)     print(f\"[DEBUG] elapsed time: {elapsed_time:.6f} seconds\\n\")   if __name__ == \"__main__\":     print(f\"[INFO] Starlette Version: {starlette.__version__}\")     for n in [5000, 10000, 20000, 40000]:         run_once(n)  \"\"\" $ python3 poc_dos_range.py [INFO] Starlette Version: 0.48.0 [DEBUG] range_body length: 5002 bytes [DEBUG] elapsed time: 0.053932 seconds  [DEBUG] range_body length: 10002 bytes [DEBUG] elapsed time: 0.209770 seconds  [DEBUG] range_body length: 20002 bytes [DEBUG] elapsed time: 0.885296 seconds  [DEBUG] range_body length: 40002 bytes [DEBUG] elapsed time: 3.238832 seconds \"\"\" ```  ### Impact Any Starlette app serving files via FileResponse or StaticFiles; frameworks built on Starlette (e.g., FastAPI) are indirectly impacted when using file-serving endpoints. Unauthenticated remote attackers can exploit this via a single HTTP request with a crafted Range header."}]}, {"name": "threadpoolctl", "version": "3.6.0", "vulns": []}, {"name": "tomli", "version": "2.3.0", "vulns": []}, {"name": "tomli-w", "version": "1.2.0", "vulns": []}, {"name": "typing-extensions", "version": "4.13.2", "vulns": []}, {"name": "typing-inspection", "version": "0.4.1", "vulns": []}, {"name": "tzdata", "version": "2025.2", "vulns": []}, {"name": "urllib3", "version": "2.4.0", "vulns": [{"id": "CVE-2025-50182", "fix_versions": ["2.5.0"], "aliases": ["GHSA-48p4-8xcf-vxj5"], "description": "urllib3 [supports](https://urllib3.readthedocs.io/en/2.4.0/reference/contrib/emscripten.html) being used in a Pyodide runtime utilizing the [JavaScript Fetch API](https://developer.mozilla.org/en-US/docs/Web/API/Fetch_API) or falling back on [XMLHttpRequest](https://developer.mozilla.org/en-US/docs/Web/API/XMLHttpRequest). This means you can use Python libraries to make HTTP requests from your browser or Node.js. Additionally, urllib3 provides [a mechanism](https://urllib3.readthedocs.io/en/2.4.0/user-guide.html#retrying-requests) to control redirects.  However, the `retries` and `redirect` parameters are ignored with Pyodide; the runtime itself determines redirect behavior.   ## Affected usages  Any code which relies on urllib3 to control the number of redirects for an HTTP request in a Pyodide runtime.   ## Impact  Redirects are often used to exploit SSRF vulnerabilities. An application attempting to mitigate SSRF or open redirect vulnerabilities by disabling redirects may remain vulnerable if a Pyodide runtime redirect mechanism is unsuitable.   ## Remediation  If you use urllib3 in Node.js, upgrade to a patched version of urllib3.  Unfortunately, browsers provide no suitable way which urllib3 can use: `XMLHttpRequest` provides no control over redirects, the Fetch API returns `opaqueredirect` responses lacking data when redirects are controlled manually. Expect default browser behavior for redirects."}, {"id": "CVE-2025-50181", "fix_versions": ["2.5.0"], "aliases": ["GHSA-pq67-6m6q-mj2v"], "description": "urllib3 handles redirects and retries using the same mechanism, which is controlled by the `Retry` object. The most common way to disable redirects is at the request level, as follows:  ```python resp = urllib3.request(\"GET\", \"https://httpbin.org/redirect/1\", redirect=False) print(resp.status) # 302 ```  However, it is also possible to disable redirects, for all requests, by instantiating a `PoolManager` and specifying `retries` in a way that disable redirects:  ```python import urllib3  http = urllib3.PoolManager(retries=0)  # should raise MaxRetryError on redirect http = urllib3.PoolManager(retries=urllib3.Retry(redirect=0))  # equivalent to the above http = urllib3.PoolManager(retries=False)  # should return the first response  resp = http.request(\"GET\", \"https://httpbin.org/redirect/1\") ```  However, the `retries` parameter is currently ignored, which means all the above examples don't disable redirects.  ## Affected usages  Passing `retries` on `PoolManager` instantiation to disable redirects or restrict their number.  By default, requests and botocore users are not affected.  ## Impact  Redirects are often used to exploit SSRF vulnerabilities. An application attempting to mitigate SSRF or open redirect vulnerabilities by disabling redirects at the PoolManager level will remain vulnerable.  ## Remediation  You can remediate this vulnerability with the following steps:   * Upgrade to a patched version of urllib3. If your organization would benefit from the continued support of urllib3 1.x, please contact [sethmichaellarson@gmail.com](mailto:sethmichaellarson@gmail.com) to discuss sponsorship or contribution opportunities.  * Disable redirects at the `request()` level instead of the `PoolManager()` level."}, {"id": "CVE-2025-66418", "fix_versions": ["2.6.0"], "aliases": ["GHSA-gm62-xv2j-4w53"], "description": "## Impact  urllib3 supports chained HTTP encoding algorithms for response content according to RFC 9110 (e.g., `Content-Encoding: gzip, zstd`).  However, the number of links in the decompression chain was unbounded allowing a malicious server to insert a virtually unlimited number of compression steps leading to high CPU usage and massive memory allocation for the decompressed data.   ## Affected usages  Applications and libraries using urllib3 version 2.5.0 and earlier for HTTP requests to untrusted sources unless they disable content decoding explicitly.   ## Remediation  Upgrade to at least urllib3 v2.6.0 in which the library limits the number of links to 5.  If upgrading is not immediately possible, use [`preload_content=False`](https://urllib3.readthedocs.io/en/2.5.0/advanced-usage.html#streaming-and-i-o) and ensure that `resp.headers[\"content-encoding\"]` contains a safe number of encodings before reading the response content."}, {"id": "CVE-2025-66471", "fix_versions": ["2.6.0"], "aliases": ["GHSA-2xpw-w6gg-jr37"], "description": "### Impact  urllib3's [streaming API](https://urllib3.readthedocs.io/en/2.5.0/advanced-usage.html#streaming-and-i-o) is designed for the efficient handling of large HTTP responses by reading the content in chunks, rather than loading the entire response body into memory at once.  When streaming a compressed response, urllib3 can perform decoding or decompression based on the HTTP `Content-Encoding` header (e.g., `gzip`, `deflate`, `br`, or `zstd`). The library must read compressed data from the network and decompress it until the requested chunk size is met. Any resulting decompressed data that exceeds the requested amount is held in an internal buffer for the next read operation.  The decompression logic could cause urllib3 to fully decode a small amount of highly compressed data in a single operation. This can result in excessive resource consumption (high CPU usage and massive memory allocation for the decompressed data; CWE-409) on the client side, even if the application only requested a small chunk of data.   ### Affected usages  Applications and libraries using urllib3 version 2.5.0 and earlier to stream large compressed responses or content from untrusted sources.  `stream()`, `read(amt=256)`, `read1(amt=256)`, `read_chunked(amt=256)`, `readinto(b)` are examples of `urllib3.HTTPResponse` method calls using the affected logic unless decoding is disabled explicitly.   ### Remediation  Upgrade to at least urllib3 v2.6.0 in which the library avoids decompressing data that exceeds the requested amount.  If your environment contains a package facilitating the Brotli encoding, upgrade to at least Brotli 1.2.0 or brotlicffi 1.2.0.0 too. These versions are enforced by the `urllib3[brotli]` extra in the patched versions of urllib3.   ### Credits  The issue was reported by @Cycloctane. Supplemental information was provided by @stamparm during a security audit performed by [7ASecurity](https://7asecurity.com/) and facilitated by [OSTIF](https://ostif.org/)."}, {"id": "CVE-2026-21441", "fix_versions": ["2.6.3"], "aliases": ["GHSA-38jv-5279-wg99"], "description": "### Impact  urllib3's [streaming API](https://urllib3.readthedocs.io/en/2.6.2/advanced-usage.html#streaming-and-i-o) is designed for the efficient handling of large HTTP responses by reading the content in chunks, rather than loading the entire response body into memory at once.  urllib3 can perform decoding or decompression based on the HTTP `Content-Encoding` header (e.g., `gzip`, `deflate`, `br`, or `zstd`). When using the streaming API, the library decompresses only the necessary bytes, enabling partial content consumption.  However, for HTTP redirect responses, the library would read the entire response body to drain the connection and decompress the content unnecessarily. This decompression occurred even before any read methods were called, and configured read limits did not restrict the amount of decompressed data. As a result, there was no safeguard against decompression bombs. A malicious server could exploit this to trigger excessive resource consumption on the client (high CPU usage and large memory allocations for decompressed data; CWE-409).  ### Affected usages  Applications and libraries using urllib3 version 2.6.2 and earlier to stream content from untrusted sources by setting `preload_content=False` when they do not disable redirects.   ### Remediation  Upgrade to at least urllib3 v2.6.3 in which the library does not decode content of redirect responses when `preload_content=False`.  If upgrading is not immediately possible, disable [redirects](https://urllib3.readthedocs.io/en/2.6.2/user-guide.html#retrying-requests) by setting `redirect=False` for requests to untrusted source."}]}, {"name": "uvicorn", "version": "0.34.3", "vulns": []}, {"name": "wrapt", "version": "2.0.1", "vulns": []}, {"name": "yarl", "version": "1.22.0", "vulns": []}], "fixes": []}
