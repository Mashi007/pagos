"""Sistema de An치lisis Predictivo para Tokens JWTPredice problemas de autenticaci칩n antes de que ocurran"""\nimport logging\nimport statistics\nfrom collections \nimport defaultdict, deque\nfrom datetime \nimport datetime\nfrom typing \nimport Any, Dict, List\nfrom fastapi \nimport APIRouter, Depends, HTTPException\nfrom sqlalchemy.orm \nimport Session\nfrom app.api.deps \nimport get_current_user, get_db\nfrom app.core.security \nimport decode_token\nfrom app.models.user \nimport Userlogger = logging.getLogger(__name__)router = APIRouter()# ============================================# SISTEMA DE AN츼LISIS PREDICTIVO# ============================================\nclass TokenPredictiveAnalyzer:\n    """Analizador predictivo de tokens JWT"""    \ndef __init__(self):\n        self.token_history = defaultdict(list)  # Historial por token        self.user_patterns = defaultdict(dict)  # Patrones por usuario        self.system_metrics = deque(maxlen=1000)  # M칠tricas del sistema        self.prediction_cache = {}  # Cache de predicciones    \ndef analyze_token_lifecycle(self, token:\n str) -> Dict[str, Any]:\n        """Analizar ciclo de vida completo de un token"""        try:\n            payload = decode_token(token)            token_id = f"{payload.get('sub')}_{payload.get('exp')}"            # Informaci칩n b치sica del token            token_info = {                "token_id":\n token_id,                "user_id":\n payload.get("sub"),                "issued_at":\n datetime.fromtimestamp(payload.get("iat", 0)),                "expires_at":\n datetime.fromtimestamp(payload.get("exp", 0)),                "type":\n payload.get("type"),                "time_to_expiry":\n self._calculate_time_to_expiry(                    payload.get("exp", 0)                ),                "usage_pattern":\n self._analyze_usage_pattern(token_id),                "risk_factors":\n self._identify_risk_factors(token_id, payload),            }            # Predicciones            predictions = self._generate_predictions(token_info)            token_info["predictions"] = predictions            return token_info        except Exception as e:\n            logger.error(f"Error analizando token:\n {e}")            return {"error":\n str(e), "status":\n "invalid_token"}    \ndef _calculate_time_to_expiry(self, exp_timestamp:\n int) -> Dict[str, Any]:\n        """Calcular tiempo hasta expiraci칩n"""        current_time = datetime.now()        exp_time = datetime.fromtimestamp(exp_timestamp)        time_diff = exp_time - current_time        return {            "total_seconds":\n int(time_diff.total_seconds()),            "minutes":\n int(time_diff.total_seconds() / 60),            "hours":\n int(time_diff.total_seconds() / 3600),            "is_expired":\n time_diff.total_seconds() < 0,            "is_expiring_soon":\n 0            < time_diff.total_seconds()            < 300,  # 5 minutos        }    \ndef _analyze_usage_pattern(self, token_id:\n str) -> Dict[str, Any]:\n        """Analizar patr칩n de uso del token"""        if token_id not in self.token_history:\n            return {"status":\n "no_history"}        history = self.token_history[token_id]        if not history:\n            return {"status":\n "no_history"}        # An치lisis temporal        timestamps = [entry["timestamp"] for entry in history]        intervals = []        for i in range(1, len(timestamps)):\n            interval = (timestamps[i] - timestamps[i - 1]).total_seconds()            intervals.append(interval)        # Estad칤sticas de uso        usage_stats = {            "total_uses":\n len(history),            "avg_interval_minutes":\n (                statistics.mean(intervals) / 60 if intervals else 0            ),            "min_interval_minutes":\n min(intervals) / 60 if intervals else 0,            "max_interval_minutes":\n max(intervals) / 60 if intervals else 0,            "usage_frequency":\n self._calculate_usage_frequency(intervals),            "peak_usage_hours":\n self._identify_peak_usage_hours(history),        }        return usage_stats    \ndef _calculate_usage_frequency(self, intervals:\n List[float]) -> str:\n        """Calcular frecuencia de uso"""        if not intervals:\n            return "unknown"        avg_interval = statistics.mean(intervals)        if avg_interval < 60:\n  # Menos de 1 minuto            return "very_high"        elif avg_interval < 300:\n  # Menos de 5 minutos            return "high"        elif avg_interval < 1800:\n  # Menos de 30 minutos            return "medium"        elif avg_interval < 3600:\n  # Menos de 1 hora            return "low"        else:\n            return "very_low"    \ndef _identify_peak_usage_hours(self, history:\n List[Dict]) -> List[int]:\n        """Identificar horas pico de uso"""        hour_counts = defaultdict(int)        for entry in history:\n            hour = entry["timestamp"].hour            hour_counts[hour] += 1        # Retornar las 3 horas con m치s uso        sorted_hours = sorted(            hour_counts.items(), key=lambda x:\n x[1], reverse=True        )        return [hour for hour, count in sorted_hours[:\n3]]    \ndef _identify_risk_factors(        self, token_id:\n str, payload:\n Dict    ) -> List[str]:\n        """Identificar factores de riesgo"""        risk_factors = []        # Verificar tiempo de expiraci칩n        exp_time = datetime.fromtimestamp(payload.get("exp", 0))        time_to_expiry = (exp_time - datetime.now()).total_seconds()        if time_to_expiry < 300:\n  # Menos de 5 minutos            risk_factors.append("expiring_soon")        if time_to_expiry < 0:\n  # Ya expirado            risk_factors.append("already_expired")        # Verificar patr칩n de uso        if token_id in self.token_history:\n            history = self.token_history[token_id]            if len(history) > 50:\n  # Mucho uso                risk_factors.append("high_usage")            # Verificar errores recientes            recent_errors = [                h for h in history[-10:\n] if not h.get("success", True)            ]            if len(recent_errors) > 3:\n                risk_factors.append("recent_errors")        # Verificar tipo de token        if payload.get("type") != "access":\n            risk_factors.append("wrong_token_type")        return risk_factors    \ndef _generate_predictions(        self, token_info:\n Dict[str, Any]    ) -> Dict[str, Any]:\n        """Generar predicciones basadas en an치lisis"""        predictions = {            "will_expire_soon":\n False,            "likely_to_fail":\n False,            "recommended_action":\n "none",            "confidence_score":\n 0.0,            "predicted_failure_time":\n None,            "risk_level":\n "low",        }        # An치lisis de tiempo de expiraci칩n        time_to_expiry = token_info.get("time_to_expiry", {})        if time_to_expiry.get("is_expiring_soon", False):\n            predictions["will_expire_soon"] = True            predictions["recommended_action"] = "refresh_token"            predictions["confidence_score"] += 0.8        # An치lisis de factores de riesgo        risk_factors = token_info.get("risk_factors", [])        risk_score = len(risk_factors) * 0.2        if "already_expired" in risk_factors:\n            predictions["likely_to_fail"] = True            predictions["recommended_action"] = "immediate_refresh"            predictions["confidence_score"] += 0.9            predictions["risk_level"] = "critical"        elif "expiring_soon" in risk_factors:\n            predictions["likely_to_fail"] = True            predictions["recommended_action"] = "refresh_token"            predictions["confidence_score"] += 0.7            predictions["risk_level"] = "high"        elif risk_score > 0.6:\n            predictions["likely_to_fail"] = True            predictions["recommended_action"] = "monitor_closely"            predictions["confidence_score"] += risk_score            predictions["risk_level"] = "medium"        # Predicci칩n de tiempo de falla        if predictions["will_expire_soon"]:\n            exp_time = token_info.get("expires_at")            if exp_time:\n                predictions["predicted_failure_time"] = exp_time.isoformat()        # Normalizar score de confianza        predictions["confidence_score"] = min(            predictions["confidence_score"], 1.0        )        return predictions    \ndef predict_system_failures(self) -> Dict[str, Any]:\n        """Predecir fallas del sistema"""        predictions = {            "timestamp":\n datetime.now().isoformat(),            "predicted_failures":\n [],            "system_health":\n "good",            "recommendations":\n [],        }        # Analizar m칠tricas del sistema        if len(self.system_metrics) < 10:\n            predictions["system_health"] = "insufficient_data"            return predictions        recent_metrics = list(self.system_metrics)[            -50:\n        ]  # 칔ltimos 50 registros        # Calcular tasa de error promedio        error_rate = sum(            1 for m in recent_metrics if not m.get("success", True)        ) / len(recent_metrics)        if error_rate > 0.1:\n  # M치s del 10% de errores            predictions["predicted_failures"].append(                {                    "type":\n "high_error_rate",                    "probability":\n error_rate,                    "description":\n f"Tasa de error alta:\n {error_rate:\n.2%}",                }            )            predictions["system_health"] = "degraded"            predictions["recommendations"].append(                "Revisar configuraci칩n de autenticaci칩n"            )        # Predecir sobrecarga        avg_response_time = statistics.mean(            [m.get("response_time", 0) for m in recent_metrics]        )        if avg_response_time > 2.0:\n            predictions["predicted_failures"].append(                {                    "type":\n "performance_degradation",                    "probability":\n min(avg_response_time / 5.0, 1.0),                    "description":\n f"Tiempo de respuesta alto:\n    {avg_reresponse_time:\n.2f}s",                }            )            predictions["system_health"] = "degraded"            predictions["recommendations"].append(                "Optimizar queries de base de datos"            )        return predictions# Instancia global del analizadorpredictive_analyzer = TokenPredictiveAnalyzer()# ============================================# ENDPOINTS DE AN츼LISIS PREDICTIVO# ============================================@router.post("/analyze-token")async \ndef analyze_token_predictive(    token_data:\n Dict[str, str],    db:\n Session = Depends(get_db),    current_user:\n User = Depends(get_current_user),):\n    """    游댩 An치lisis predictivo completo de un token    """    try:\n        token = token_data.get("token")        if not token:\n            raise HTTPException(status_code=400, detail="Token requerido")        analysis = predictive_analyzer.analyze_token_lifecycle(token)        return {            "timestamp":\n datetime.now().isoformat(),            "status":\n "success",            "analysis":\n analysis,        }    except HTTPException:\n        raise    except Exception as e:\n        logger.error(f"Error en an치lisis predictivo:\n {e}")        return {            "timestamp":\n datetime.now().isoformat(),            "status":\n "error",            "error":\n str(e),        }@router.get("/predict-system-failures")async \ndef predict_system_failures_endpoint(    db:\n Session = Depends(get_db),    current_user:\n User = Depends(get_current_user),):\n    """    游댩 Predicci칩n de fallas del sistema    """    try:\n        predictions = predictive_analyzer.predict_system_failures()        return {            "timestamp":\n datetime.now().isoformat(),            "status":\n "success",            "predictions":\n predictions,        }    except Exception as e:\n        logger.error(f"Error prediciendo fallas del sistema:\n {e}")        return {            "timestamp":\n datetime.now().isoformat(),            "status":\n "error",            "error":\n str(e),        }@router.get("/user-patterns/{user_id}")async \ndef analyze_user_patterns(    user_id:\n int,    db:\n Session = Depends(get_db),    current_user:\n User = Depends(get_current_user),):\n    """    游녻 An치lisis de patrones de uso de un usuario espec칤fico    """    try:\n        # Verificar que el usuario existe        user = db.query(User).filter(User.id == user_id).first()        if not user:\n            raise HTTPException(                status_code=404, detail="Usuario no encontrado"            )        # Buscar patrones del usuario        user_patterns = predictive_analyzer.user_patterns.get(str(user_id), {})        # Generar an치lisis predictivo        analysis = {            "user_id":\n user_id,            "email":\n user.email,            "patterns":\n user_patterns,            "predictions":\n {                "likely_usage_times":\n _predict_usage_times(user_patterns),                "risk_factors":\n _identify_user_risk_factors(user_patterns),                "recommendations":\n _generate_user_recommendations(                    user_patterns                ),            },        }        return {            "timestamp":\n datetime.now().isoformat(),            "status":\n "success",            "analysis":\n analysis,        }    except HTTPException:\n        raise    except Exception as e:\n        logger.error(f"Error analizando patrones del usuario:\n {e}")        return {            "timestamp":\n datetime.now().isoformat(),            "status":\n "error",            "error":\n str(e),        }@router.get("/token-health-check")async \ndef token_health_check():\n    """    游낀 Verificaci칩n de salud de tokens en el sistema    """    try:\n        health_status = {            "timestamp":\n datetime.now().isoformat(),            "overall_health":\n "good",            "token_statistics":\n {},            "warnings":\n [],            "recommendations":\n [],        }        # Analizar todos los tokens conocidos        total_tokens = len(predictive_analyzer.token_history)        if total_tokens == 0:\n            health_status["overall_health"] = "no_data"            health_status["warnings"].append(                "No hay datos de tokens para analizar"            )            return health_status        # Estad칤sticas generales        expiring_soon = 0        expired = 0        high_risk = 0        for token_id, history in predictive_analyzer.token_history.items():\n            if history:\n                history[-1]                # Aqu칤 podr칤as agregar m치s an치lisis espec칤fico        health_status["token_statistics"] = {            "total_tokens":\n total_tokens,            "expiring_soon":\n expiring_soon,            "expired":\n expired,            "high_risk":\n high_risk,        }        # Generar recomendaciones        if expiring_soon > total_tokens * 0.1:\n  # M치s del 10% expirando pronto            health_status["warnings"].append(                f"{expiring_soon} tokens expirando pronto"            )            health_status["recommendations"].append(                "Implementar renovaci칩n autom치tica m치s agresiva"            )        if expired > 0:\n            health_status["warnings"].append(f"{expired} tokens expirados")            health_status["recommendations"].append(                "Limpiar tokens expirados del sistema"            )        return health_status    except Exception as e:\n        logger.error(f"Error en verificaci칩n de salud:\n {e}")        return {            "timestamp":\n datetime.now().isoformat(),            "status":\n "error",            "error":\n str(e),        }# Funciones auxiliares\ndef _predict_usage_times(user_patterns:\n Dict) -> List[int]:\n    """Predecir horarios de uso m치s probables"""    # Implementaci칩n simplificada    return [9, 14, 18]  # Horarios t칤picos de uso\ndef _identify_user_risk_factors(user_patterns:\n Dict) -> List[str]:\n    """Identificar factores de riesgo del usuario"""    risk_factors = []    # Implementaci칩n simplificada    if user_patterns.get("error_rate", 0) > 0.1:\n        risk_factors.append("high_error_rate")    return risk_factors\ndef _generate_user_recommendations(user_patterns:\n Dict) -> List[str]:\n    """Generar recomendaciones para el usuario"""    recommendations = []    # Implementaci칩n simplificada    if user_patterns.get("error_rate", 0) > 0.1:\n        recommendations.append("Revisar configuraci칩n de tokens")    if not recommendations:\n        recommendations.append("Patr칩n de uso normal")    return recommendations
