"""Sistema de Análisis Predictivo para Tokens JWTPredice problemas de autenticación antes de que ocurran"""\nimport logging\nimport statistics\nfrom collections \nimport defaultdict, deque\nfrom datetime \nimport datetime\nfrom typing \nimport Any, Dict, List\nfrom fastapi \nimport APIRouter, Depends, HTTPException\nfrom sqlalchemy.orm \nimport Session\nfrom app.api.deps \nimport get_current_user, get_db\nfrom app.core.security \nimport decode_token\nfrom app.models.user \nimport Userlogger = logging.getLogger(__name__)router = APIRouter()# ============================================# SISTEMA DE ANÁLISIS PREDICTIVO# ============================================\nclass TokenPredictiveAnalyzer:\n    """Analizador predictivo de tokens JWT"""    \ndef __init__(self):\n        self.token_history = defaultdict(list)  # Historial por token        self.user_patterns = defaultdict(dict)  # Patrones por usuario        self.system_metrics = deque(maxlen=1000)  # Métricas del sistema        self.prediction_cache = {}  # Cache de predicciones    \ndef analyze_token_lifecycle(self, token:\n str) -> Dict[str, Any]:\n        """Analizar ciclo de vida completo de un token"""        try:\n            payload = decode_token(token)            token_id = f"{payload.get('sub')}_{payload.get('exp')}"            # Información básica del token            token_info = {                "token_id":\n token_id,                "user_id":\n payload.get("sub"),                "issued_at":\n datetime.fromtimestamp(payload.get("iat", 0)),                "expires_at":\n datetime.fromtimestamp(payload.get("exp", 0)),                "type":\n payload.get("type"),                "time_to_expiry":\n self._calculate_time_to_expiry(                    payload.get("exp", 0)                ),                "usage_pattern":\n self._analyze_usage_pattern(token_id),                "risk_factors":\n self._identify_risk_factors(token_id, payload),            }            # Predicciones            predictions = self._generate_predictions(token_info)            token_info["predictions"] = predictions            return token_info        except Exception as e:\n            logger.error(f"Error analizando token:\n {e}")            return {"error":\n str(e), "status":\n "invalid_token"}    \ndef _calculate_time_to_expiry(self, exp_timestamp:\n int) -> Dict[str, Any]:\n        """Calcular tiempo hasta expiración"""        current_time = datetime.now()        exp_time = datetime.fromtimestamp(exp_timestamp)        time_diff = exp_time - current_time        return {            "total_seconds":\n int(time_diff.total_seconds()),            "minutes":\n int(time_diff.total_seconds() / 60),            "hours":\n int(time_diff.total_seconds() / 3600),            "is_expired":\n time_diff.total_seconds() < 0,            "is_expiring_soon":\n 0            < time_diff.total_seconds()            < 300,  # 5 minutos        }    \ndef _analyze_usage_pattern(self, token_id:\n str) -> Dict[str, Any]:\n        """Analizar patrón de uso del token"""        if token_id not in self.token_history:\n            return {"status":\n "no_history"}        history = self.token_history[token_id]        if not history:\n            return {"status":\n "no_history"}        # Análisis temporal        timestamps = [entry["timestamp"] for entry in history]        intervals = []        for i in range(1, len(timestamps)):\n            interval = (timestamps[i] - timestamps[i - 1]).total_seconds()            intervals.append(interval)        # Estadísticas de uso        usage_stats = {            "total_uses":\n len(history),            "avg_interval_minutes":\n (                statistics.mean(intervals) / 60 if intervals else 0            ),            "min_interval_minutes":\n min(intervals) / 60 if intervals else 0,            "max_interval_minutes":\n max(intervals) / 60 if intervals else 0,            "usage_frequency":\n self._calculate_usage_frequency(intervals),            "peak_usage_hours":\n self._identify_peak_usage_hours(history),        }        return usage_stats    \ndef _calculate_usage_frequency(self, intervals:\n List[float]) -> str:\n        """Calcular frecuencia de uso"""        if not intervals:\n            return "unknown"        avg_interval = statistics.mean(intervals)        if avg_interval < 60:\n  # Menos de 1 minuto            return "very_high"        elif avg_interval < 300:\n  # Menos de 5 minutos            return "high"        elif avg_interval < 1800:\n  # Menos de 30 minutos            return "medium"        elif avg_interval < 3600:\n  # Menos de 1 hora            return "low"        else:\n            return "very_low"    \ndef _identify_peak_usage_hours(self, history:\n List[Dict]) -> List[int]:\n        """Identificar horas pico de uso"""        hour_counts = defaultdict(int)        for entry in history:\n            hour = entry["timestamp"].hour            hour_counts[hour] += 1        # Retornar las 3 horas con más uso        sorted_hours = sorted(            hour_counts.items(), key=lambda x:\n x[1], reverse=True        )        return [hour for hour, count in sorted_hours[:\n3]]    \ndef _identify_risk_factors(        self, token_id:\n str, payload:\n Dict    ) -> List[str]:\n        """Identificar factores de riesgo"""        risk_factors = []        # Verificar tiempo de expiración        exp_time = datetime.fromtimestamp(payload.get("exp", 0))        time_to_expiry = (exp_time - datetime.now()).total_seconds()        if time_to_expiry < 300:\n  # Menos de 5 minutos            risk_factors.append("expiring_soon")        if time_to_expiry < 0:\n  # Ya expirado            risk_factors.append("already_expired")        # Verificar patrón de uso        if token_id in self.token_history:\n            history = self.token_history[token_id]            if len(history) > 50:\n  # Mucho uso                risk_factors.append("high_usage")            # Verificar errores recientes            recent_errors = [                h for h in history[-10:\n] if not h.get("success", True)            ]            if len(recent_errors) > 3:\n                risk_factors.append("recent_errors")        # Verificar tipo de token        if payload.get("type") != "access":\n            risk_factors.append("wrong_token_type")        return risk_factors    \ndef _generate_predictions(        self, token_info:\n Dict[str, Any]    ) -> Dict[str, Any]:\n        """Generar predicciones basadas en análisis"""        predictions = {            "will_expire_soon":\n False,            "likely_to_fail":\n False,            "recommended_action":\n "none",            "confidence_score":\n 0.0,            "predicted_failure_time":\n None,            "risk_level":\n "low",        }        # Análisis de tiempo de expiración        time_to_expiry = token_info.get("time_to_expiry", {})        if time_to_expiry.get("is_expiring_soon", False):\n            predictions["will_expire_soon"] = True            predictions["recommended_action"] = "refresh_token"            predictions["confidence_score"] += 0.8        # Análisis de factores de riesgo        risk_factors = token_info.get("risk_factors", [])        risk_score = len(risk_factors) * 0.2        if "already_expired" in risk_factors:\n            predictions["likely_to_fail"] = True            predictions["recommended_action"] = "immediate_refresh"            predictions["confidence_score"] += 0.9            predictions["risk_level"] = "critical"        elif "expiring_soon" in risk_factors:\n            predictions["likely_to_fail"] = True            predictions["recommended_action"] = "refresh_token"            predictions["confidence_score"] += 0.7            predictions["risk_level"] = "high"        elif risk_score > 0.6:\n            predictions["likely_to_fail"] = True            predictions["recommended_action"] = "monitor_closely"            predictions["confidence_score"] += risk_score            predictions["risk_level"] = "medium"        # Predicción de tiempo de falla        if predictions["will_expire_soon"]:\n            exp_time = token_info.get("expires_at")            if exp_time:\n                predictions["predicted_failure_time"] = exp_time.isoformat()        # Normalizar score de confianza        predictions["confidence_score"] = min(            predictions["confidence_score"], 1.0        )        return predictions    \ndef predict_system_failures(self) -> Dict[str, Any]:\n        """Predecir fallas del sistema"""        predictions = {            "timestamp":\n datetime.now().isoformat(),            "predicted_failures":\n [],            "system_health":\n "good",            "recommendations":\n [],        }        # Analizar métricas del sistema        if len(self.system_metrics) < 10:\n            predictions["system_health"] = "insufficient_data"            return predictions        recent_metrics = list(self.system_metrics)[            -50:\n        ]  # Últimos 50 registros        # Calcular tasa de error promedio        error_rate = sum(            1 for m in recent_metrics if not m.get("success", True)        ) / len(recent_metrics)        if error_rate > 0.1:\n  # Más del 10% de errores            predictions["predicted_failures"].append(                {                    "type":\n "high_error_rate",                    "probability":\n error_rate,                    "description":\n f"Tasa de error alta:\n {error_rate:\n.2%}",                }            )            predictions["system_health"] = "degraded"            predictions["recommendations"].append(                "Revisar configuración de autenticación"            )        # Predecir sobrecarga        avg_response_time = statistics.mean(            [m.get("response_time", 0) for m in recent_metrics]        )        if avg_response_time > 2.0:\n            predictions["predicted_failures"].append(                {                    "type":\n "performance_degradation",                    "probability":\n min(avg_response_time / 5.0, 1.0),                    "description":\n f"Tiempo de respuesta alto:\n    {avg_reresponse_time:\n.2f}s",                }            )            predictions["system_health"] = "degraded"            predictions["recommendations"].append(                "Optimizar queries de base de datos"            )        return predictions# Instancia global del analizadorpredictive_analyzer = TokenPredictiveAnalyzer()# ============================================# ENDPOINTS DE ANÁLISIS PREDICTIVO# ============================================@router.post("/analyze-token")async \ndef analyze_token_predictive(    token_data:\n Dict[str, str],    db:\n Session = Depends(get_db),    current_user:\n User = Depends(get_current_user),):\n    """    🔮 Análisis predictivo completo de un token    """    try:\n        token = token_data.get("token")        if not token:\n            raise HTTPException(status_code=400, detail="Token requerido")        analysis = predictive_analyzer.analyze_token_lifecycle(token)        return {            "timestamp":\n datetime.now().isoformat(),            "status":\n "success",            "analysis":\n analysis,        }    except HTTPException:\n        raise    except Exception as e:\n        logger.error(f"Error en análisis predictivo:\n {e}")        return {            "timestamp":\n datetime.now().isoformat(),            "status":\n "error",            "error":\n str(e),        }@router.get("/predict-system-failures")async \ndef predict_system_failures_endpoint(    db:\n Session = Depends(get_db),    current_user:\n User = Depends(get_current_user),):\n    """    🔮 Predicción de fallas del sistema    """    try:\n        predictions = predictive_analyzer.predict_system_failures()        return {            "timestamp":\n datetime.now().isoformat(),            "status":\n "success",            "predictions":\n predictions,        }    except Exception as e:\n        logger.error(f"Error prediciendo fallas del sistema:\n {e}")        return {            "timestamp":\n datetime.now().isoformat(),            "status":\n "error",            "error":\n str(e),        }@router.get("/user-patterns/{user_id}")async \ndef analyze_user_patterns(    user_id:\n int,    db:\n Session = Depends(get_db),    current_user:\n User = Depends(get_current_user),):\n    """    👤 Análisis de patrones de uso de un usuario específico    """    try:\n        # Verificar que el usuario existe        user = db.query(User).filter(User.id == user_id).first()        if not user:\n            raise HTTPException(                status_code=404, detail="Usuario no encontrado"            )        # Buscar patrones del usuario        user_patterns = predictive_analyzer.user_patterns.get(str(user_id), {})        # Generar análisis predictivo        analysis = {            "user_id":\n user_id,            "email":\n user.email,            "patterns":\n user_patterns,            "predictions":\n {                "likely_usage_times":\n _predict_usage_times(user_patterns),                "risk_factors":\n _identify_user_risk_factors(user_patterns),                "recommendations":\n _generate_user_recommendations(                    user_patterns                ),            },        }        return {            "timestamp":\n datetime.now().isoformat(),            "status":\n "success",            "analysis":\n analysis,        }    except HTTPException:\n        raise    except Exception as e:\n        logger.error(f"Error analizando patrones del usuario:\n {e}")        return {            "timestamp":\n datetime.now().isoformat(),            "status":\n "error",            "error":\n str(e),        }@router.get("/token-health-check")async \ndef token_health_check():\n    """    🏥 Verificación de salud de tokens en el sistema    """    try:\n        health_status = {            "timestamp":\n datetime.now().isoformat(),            "overall_health":\n "good",            "token_statistics":\n {},            "warnings":\n [],            "recommendations":\n [],        }        # Analizar todos los tokens conocidos        total_tokens = len(predictive_analyzer.token_history)        if total_tokens == 0:\n            health_status["overall_health"] = "no_data"            health_status["warnings"].append(                "No hay datos de tokens para analizar"            )            return health_status        # Estadísticas generales        expiring_soon = 0        expired = 0        high_risk = 0        for token_id, history in predictive_analyzer.token_history.items():\n            if history:\n                history[-1]                # Aquí podrías agregar más análisis específico        health_status["token_statistics"] = {            "total_tokens":\n total_tokens,            "expiring_soon":\n expiring_soon,            "expired":\n expired,            "high_risk":\n high_risk,        }        # Generar recomendaciones        if expiring_soon > total_tokens * 0.1:\n  # Más del 10% expirando pronto            health_status["warnings"].append(                f"{expiring_soon} tokens expirando pronto"            )            health_status["recommendations"].append(                "Implementar renovación automática más agresiva"            )        if expired > 0:\n            health_status["warnings"].append(f"{expired} tokens expirados")            health_status["recommendations"].append(                "Limpiar tokens expirados del sistema"            )        return health_status    except Exception as e:\n        logger.error(f"Error en verificación de salud:\n {e}")        return {            "timestamp":\n datetime.now().isoformat(),            "status":\n "error",            "error":\n str(e),        }# Funciones auxiliares\ndef _predict_usage_times(user_patterns:\n Dict) -> List[int]:\n    """Predecir horarios de uso más probables"""    # Implementación simplificada    return [9, 14, 18]  # Horarios típicos de uso\ndef _identify_user_risk_factors(user_patterns:\n Dict) -> List[str]:\n    """Identificar factores de riesgo del usuario"""    risk_factors = []    # Implementación simplificada    if user_patterns.get("error_rate", 0) > 0.1:\n        risk_factors.append("high_error_rate")    return risk_factors\ndef _generate_user_recommendations(user_patterns:\n Dict) -> List[str]:\n    """Generar recomendaciones para el usuario"""    recommendations = []    # Implementación simplificada    if user_patterns.get("error_rate", 0) > 0.1:\n        recommendations.append("Revisar configuración de tokens")    if not recommendations:\n        recommendations.append("Patrón de uso normal")    return recommendations
